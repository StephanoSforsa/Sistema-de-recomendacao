# -*- coding: utf-8 -*-
"""Dinamica day 9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13ylG2viA3NhXYVfa16rSjXz4Ke3Rfyih

# Modelos de recomendações:

##Modelo de recomendação por avaliação:

###Carregando as bases e analisando:
"""

import pandas as pd

dnc_order_reviews = pd.read_csv('/content/DNC_order_reviews_dataset.csv',index_col=0)
dnc_order_items = pd.read_csv('/content/DNC_order_items_dataset.csv',index_col=0)
dnc_orders = pd.read_csv('/content/DNC_orders_dataset.csv',index_col=0)
dnc_products = pd.read_csv('/content/DNC_products_dataset.csv',index_col=0)

dnc_order_reviews.head()

dnc_order_items.head()

dnc_orders.head()

dnc_products.head()

"""###Tratamento de nulos, caso presentes:"""

dnc_order_reviews.info()

dnc_order_items.info()

dnc_orders.info()

dnc_products.info()

dnc_products.isnull().sum()

dnc_products = dnc_products.dropna()

df_principal = dnc_orders[dnc_orders['order_status'] == 'delivered']

df_principal = pd.merge(df_principal,dnc_order_reviews, how='inner', on='order_id')

df_principal.head()

df_principal.shape

"""###Montando o dataframe principal com as informações necessárias para o modelo:"""

dnc_order_items.columns

item_id = dnc_order_items[['order_id','product_id','order_item_id']]

df_principal = pd.merge(df_principal,item_id, how='left', on='order_id')

df_principal.shape

df_principal = df_principal.drop_duplicates('order_id')

dnc_products.columns

categorias = dnc_products[['product_id', 'product_category_name']]

df_principal = pd.merge(df_principal,categorias, how='left', on='product_id')

df_principal["product_category_name"] = df_principal["product_category_name"].astype('category')

df_principal["cat_codes"] = df_principal["product_category_name"].cat.codes

df_principal

df_principal['cat_codes']

df_principal['cat_codes'] = df_principal['cat_codes'].replace(0,73)

df_principal['cat_codes'] = df_principal['cat_codes'].replace(-1,74)

df_principal

df_principal = df_principal[['customer_id','product_id', 'review_score','order_id', 'order_status', 'review_id', 'order_item_id', 'product_category_name', 'cat_codes']]

df_principal

df_principal['review_score'].unique()

df_principal

df_principal = df_principal.dropna()

df_principal.info()

df_principal

df_principal.columns

"""###Filtrando o dataframe principal pelo número de avaliações de produtos. Adotamos um limite para analisar somente produtos que foram analisados mais de 10 vezes."""

contagem = pd.DataFrame(df_principal['product_id'].value_counts())

contagem = contagem.rename({'product_id':'contagem'},axis=1)

contagem['product_id'] = contagem.index

contagem = contagem.reset_index(drop=True)

df_principal = pd.merge(df_principal,contagem, how = 'left', on='product_id')

df_selecionados = df_principal[df_principal['contagem']>10]

df_selecionados.info()

"""### Transposição de linhas(customer_id) em colunas:"""

pivot_table = df_selecionados.pivot_table(columns='customer_id', index='product_id', values="review_score")

pivot_table

pivot_table.fillna(0, inplace=True)

pivot_table.shape

pivot_table

"""### Rodando o modelo com o NearestNeighbors:"""

from scipy.sparse import csr_matrix
book_sparse = csr_matrix(pivot_table)

from sklearn.neighbors import NearestNeighbors
model = NearestNeighbors(algorithm='brute')
model.fit(book_sparse)

"""###Avaliando as recomendações do modelo para o primeiro cliente:"""

distances, suggestions = model.kneighbors(pivot_table.iloc[0, :].values.reshape(1, -1))

pivot_table

recomendados = []
for i in range(len(suggestions)):
  recomendados= list(pivot_table.index[suggestions[i]])

for prod in recomendados:
  print('Produto:',prod +', da categoria:',df_principal['product_category_name'][df_principal['product_id']==prod].unique()[0])

"""#"""

# Importando pacotes
from surprise import KNNWithMeans
from surprise import Dataset
from surprise import accuracy
from surprise import Reader
import os
from surprise.model_selection import train_test_split
from sklearn.decomposition import TruncatedSVD

df_selecionados

# Lendo o dataset
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df_selecionados,reader)

# Split dos dados
trainset, testset = train_test_split(data, test_size=0.3,random_state=10)

# Criação de um modelo baseado em item (user_based true / false para alternar entre filtragem colaborativa baseada em usuário ou baseada em item)
algo = KNNWithMeans(k=5, sim_options={'user_based': True})
algo.fit(trainset)

# Teste do modelo
test_pred = algo.test(testset)

get RMSE
print("Item-based Model : Test Set")
accuracy.rmse(test_pred, verbose=True)

Out:
Item-based Model : Test Set 
RMSE: 1.1980

algo.get_neighbors(1, 10)

Out:
[2, 12, 17, 24, 30, 34, 35, 36, 42, 60]

# Here are the 10 recommended product names
ratings_df.iloc[[2, 12, 17, 24, 30, 34, 35, 36, 42, 60]].index

['B0000531B7', 'B0000CF8T1', 'B0000DC2VI', 'B0000DJT3C', 'B0000T71C6', 'B000100EQ4', 'B00012182G', 'B00014CZP8', 'B00014FT06', 'B00017LEXO']











def transform_sample(dataf):
  return pd.DataFrame({
      "user": dataf["customer_id"],
      "product": dataf["product_id"],
      "action": dataf["review_score"]})

def start_pipeline(dataf):
    return dataf.copy()

def get_user_counts(dataf):
  return dataf.groupby(by='customer_id', as_index=False) \
    .agg({'product_id': pd.Series.nunique}) \
    .rename(columns={'product_id': 'product_id_count'}) \
    .set_index('customer_id') \
    .sort_values('product_id_count', ascending = False)

def get_book_counts(dataf):
  return dataf.groupby(by='product_id', as_index=False) \
    .agg({'customer_id': pd.Series.nunique}) \
    .rename(columns={'customer_id': 'customer_id_count'}) \
    .set_index('product_id') \
    .sort_values('customer_id_count', ascending = False)

def get_n_top_values(dataf, n):
  return dataf.head(sample_size) \
    .index \
    .tolist()

sample_size = 1000
top_user_ids = df_principal \
  .pipe(start_pipeline) \
  .pipe(get_user_counts) \
  .pipe(get_n_top_values, sample_size)

sample_size = 999
top_products_ids = df_principal \
  .pipe(start_pipeline) \
  .pipe(get_book_counts) \
  .pipe(get_n_top_values, sample_size)



matriz_de_relacionamento_book_sampled = df_principal[df_principal["ISBN"].isin(top_book_ids)] \
  .pipe(transform_sample) \
  .pivot_table(index="user", columns="book", values="action").fillna(0)

matriz_de_relacionamento_book_sampled.T.shape

cos_item = cosine_similarity(matriz_de_relacionamento_book_sampled.T)

print(cos_item)

cos_item.shape

"""## Temos um produto e queremos saber outros produtos que são consumidos por pessoas com consumos similares"""

matriz_de_relacionamento_book_sampled.T.index

print(cos_item)

def mostra_n_mais_proximos(id_de_interesse, lista_instanciast, matriz_similaridade, n):
  print(f"Item de interesse: {instancia_de_interesse}")

  assert id_de_interesse in lista_instanciast
  cliente_idx = lista_instanciast.tolist().index(instancia_de_interesse)
  print(f"Item id: {instancia_de_interesse}, tem índice {cliente_idx}")

  closest_10_users = np.argsort(-matriz_similaridade[cliente_idx])[:n]
  print(instancia_de_interesse, closest_10_users)

  for i in zip(lista_instanciast[closest_10_users], matriz_similaridade[cliente_idx][closest_10_users]):
      print(f"Item {i[0]} tem similaridade {i[1]:.2f} com item {instancia_de_interesse}")


lista_instancias = matriz_de_relacionamento_book_sampled.T.index
instancia_de_interesse = lista_instancias[random.randint(0, len(lista_instancias))]

mostra_n_mais_proximos(instancia_de_interesse, lista_instancias, cos_item, 10)

df_principal.columns

df_principal2 = df_principal[['customer_id', 'product_id', 'cat_codes']]

"""## Modelo 2 - Item mais selem


"""

def start_pipeline(dataf):
    return dataf.copy()

def get_user_counts(dataf):
  return dataf.groupby(by='customer_id', as_index=False) \
    .agg({'product_id': pd.Series.count}) \
    .rename(columns={'product_id': 'product_id_count'}) \
    .set_index('customer_id') \
    .sort_values('product_id_count', ascending = False)

def get_book_counts(dataf):
  return dataf.groupby(by='product_id', as_index=False) \
    .agg({'customer_id': pd.Series.nunique}) \
    .rename(columns={'customer_id': 'customer_id_count'}) \
    .set_index('product_id') \
    .sort_values('customer_id_count', ascending = False)

def get_n_top_values(dataf, n):
  return dataf.head(sample_size) \
    .index \
    .tolist()

sample_size = 10000
top_user_ids = df_principal2 \
  .pipe(start_pipeline) \
  .pipe(get_user_counts) \
  .pipe(get_n_top_values, sample_size)

sample_size = 74
top_products_ids = df_principal2 \
  .pipe(start_pipeline) \
  .pipe(get_book_counts) \
  .pipe(get_n_top_values, sample_size)

def transform_sample(dataf):
  return pd.DataFrame({
      "user": dataf["customer_id"],
      "product": dataf["product_id"],
      "action": dataf["cat_codes"]
  })

matriz_de_relacionamento_book_sampled = df_principal2[df_principal2["product_id"].isin(top_products_ids)] \
  .pipe(transform_sample) \
  .pivot_table(index="user", columns="product", values="action").fillna(0)

matriz_de_relacionamento_book_sampled.T.shape

matriz_de_relacionamento_book_sampled[matriz_de_relacionamento_book_sampled['06edb72f1e0c64b14c5b79353f7abea3']>1]

from sklearn.metrics.pairwise import cosine_similarity

cos_item = cosine_similarity(matriz_de_relacionamento_book_sampled.T)

print(cos_item)

cos_item

cos_item.T.shape

import numpy as np
import random

def mostra_n_mais_proximos(id_de_interesse, lista_instanciast, matriz_similaridade, n):
  print(f"Item de interesse: {instancia_de_interesse}")

  assert id_de_interesse in lista_instanciast
  cliente_idx = lista_instanciast.tolist().index(instancia_de_interesse)
  print(f"Item id: {instancia_de_interesse}, tem índice {cliente_idx}")

  closest_10_users = np.argsort(-matriz_similaridade[cliente_idx])[:n]
  print(instancia_de_interesse, closest_10_users)

  for i in zip(lista_instanciast[closest_10_users], matriz_similaridade[cliente_idx][closest_10_users]):
      print(f"Item {i[0]} tem similaridade {i[1]:.2f} com item {instancia_de_interesse}")


lista_instancias = matriz_de_relacionamento_book_sampled.T.index
instancia_de_interesse = lista_instancias[random.randint(0, len(lista_instancias))]

mostra_n_mais_proximos(instancia_de_interesse, lista_instancias, cos_item, 10)

df_principal[df_principal['product_id']=='2b4609f8948be18874494203496bc318']

df_principal[df_principal['product_id']=='880be32f4db1d9f6e2bec38fb6ac23ab']

pivot_table = df_selecionados.pivot_table(columns='review_id', index='product_id', values="review_score")

pivot_table.fillna(0, inplace=True)

pivot_table

from sklearn.metrics.pairwise import cosine_similarity

cos_item = cosine_similarity(pivot_table)

print(cos_item)

cos_item

cos_item.T.shape

import numpy as np
import random

def mostra_n_mais_proximos(id_de_interesse, lista_instanciast, matriz_similaridade, n):
  print(f"Item de interesse: {instancia_de_interesse}")

  assert id_de_interesse in lista_instanciast
  cliente_idx = lista_instanciast.tolist().index(instancia_de_interesse)
  print(f"Item id: {instancia_de_interesse}, tem índice {cliente_idx}")

  closest_10_users = np.argsort(-matriz_similaridade[cliente_idx])[:n]
  print(instancia_de_interesse, closest_10_users)
  return lista_instanciast[closest_10_users].tolist() 


lista_instancias = matriz_de_relacionamento_book_sampled
instancia_de_interesse = lista_instancias[random.randint(0, len(lista_instancias))]

lista = mostra_n_mais_proximos(instancia_de_interesse, lista_instancias, cos_item, 10)

for

lista_instancias

len(list(instancia_de_interesse))

instancia_de_interesse